{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "# Loading dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "# Loading MiniLM\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "prompt_embeddings = embedder.encode(df['prompt'].tolist(), convert_to_tensor=True)\n",
        "\n",
        "# Loading non-GPT language model\n",
        "model_name = \"tiiuae/falcon-rw-1b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYCNQHmvoi0b",
        "outputId": "b9e4d8f6-8162-4b58-d24a-b170056b43ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_match(user_input):\n",
        "    user_embedding = embedder.encode(user_input, convert_to_tensor=True)\n",
        "    similarities = util.pytorch_cos_sim(user_embedding, prompt_embeddings)\n",
        "    best_idx = similarities.argmax().item()  # Fixed here\n",
        "    return df['prompt'].iloc[best_idx], df['response'].iloc[best_idx]\n",
        "def generate_response(user_input):\n",
        "    matched_prompt, matched_response = get_best_match(user_input)\n",
        "\n",
        "    prompt = f\"\"\"User question: {user_input}\n",
        "Related prior question: {matched_prompt}\n",
        "Answer to prior question: {matched_response}\n",
        "Now, provide a creative and informative response to the user's question:\"\"\"\n",
        "\n",
        "\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    return output.split(\"Now, provide a creative and informative response to the user's question:\")[-1].strip()"
      ],
      "metadata": {
        "id": "H9LQDu4v1BKy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        print(\"Bot:\", generate_response(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMbOUlAl1Dg0",
        "outputId": "a7f8d572-fc38-4efe-a1f5-b33f13b57d78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Who invented Light Bulb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Who invented Light Bulb?\n",
            "Answer: Thomas Edison is credited with inventing the first commercially practicable incandescent light bulb in 1879. He initially developed an incandescent light that lasted 14 hours before going out, but he gradually improved the design and by 1889, he was able to produce a bulb that could last 1,200 hours.\n",
            "(10) comments\n",
            "Thomas Edison is credited with inventing the first commercially practicable incandescent light bulb in 1879. He initially developed an\n",
            "You: Explain the story of Romeo and Juliet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Question: Explain the story of Romeo and Juliet.\n",
            "Answer:\n",
            "In the famous play Romeo and Juliet, two families, the Capulets and the Montagues, fight for the affections of a young woman named Juliet. The two families are feuding, and the feud is made worse when the families' children are engaged to be married. The feuding families, however, find that their feud has made things worse. The feud has made them enemies, and they are willing to kill\n",
            "You: Exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOeVPKe_03Oa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}